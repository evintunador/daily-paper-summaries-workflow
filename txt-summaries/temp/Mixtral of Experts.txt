In this paper, the authors introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, but with the difference that each layer is composed of 8 feedforward blocks (experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. This allows each token to have access to a large number of parameters (47B), but only uses a subset of them (13B) during inference.
`
The authors trained Mixtral with a context size of 32k tokens and evaluated its performance on various benchmarks. Mixtral outperforms or matches Llama 2 70B and GPT-3.5 on most benchmarks, including mathematics, code generation, and multilingual tasks. Mixtral also demonstrates superior capabilities in tasks that require multilingual understanding.

The authors also present Mixtral 8x7B - Instruct, a chat model fine-tuned to follow instructions. Mixtral - Instruct surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human evaluation benchmarks. It also shows reduced biases and a more balanced sentiment profile.

The results show that Mixtral achieves high performance while using fewer active parameters compared to Llama 2 70B. It also demonstrates the ability to handle long context and exhibits structured syntactic behavior in expert selection.

Critiques of the model could include the limited analysis of biases and the lack of comparison with other state-of-the-art models. Additionally, the evaluation protocol may differ from previous work, which could affect the comparison of results.

The implications of this research are significant as Mixtral provides a powerful language model architecture that can be used for various natural language processing tasks. Its performance on mathematics, code generation, and multilingual tasks opens up possibilities for applications in these domains. The release of Mixtral under the Apache 2.0 license allows for broad accessibility and potential for diverse applications.

- Transformer architecture
- Mixture of Experts (MoE) model
- Feedforward blocks
- Routing mechanism
- Language modeling
- Natural language processing
- Multilingual understanding
- Mathematics
- Code generation
- Bias evaluation
- Instruction fine-tuning

Citation: Mixtral of Experts, Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed. arXiv:2401.04088v1 [cs.LG] 8 Jan 2024.