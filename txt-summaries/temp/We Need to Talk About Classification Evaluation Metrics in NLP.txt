In this paper, the authors examine the use of classification evaluation metrics in Natural Language Processing (NLP) tasks. They compare several standard metrics, such as Accuracy, F1-Measure, and AUC-ROC, with more "exotic" metrics and propose the use of Informedness as a better metric for comparing classification performance.
`
The authors first discuss the limitations of common metrics like Accuracy and F1-Measure, which can be influenced by class prevalence and may not accurately reflect a model's true performance. They introduce Informedness, a metric that measures the proportion of times a classifier makes an informed decision better than random chance. Informedness avoids crediting models that exploit label bias or exhibit guessing behavior, making it a more reliable metric for model comparison.

To evaluate the metrics, the authors conduct experiments on a synthetic setting and a range of NLP tasks, including the GLUE benchmark and Visual Question Answering (VQA) datasets. They show that Informedness consistently provides a more interpretable and reliable measure of model performance compared to other metrics. Informedness correctly identifies the underlying probability of a model making an informed decision and allows for direct comparison between tasks with varying bias or class distributions.

The authors also highlight the limitations of other metrics, such as MCC and NIT, which may not be suitable for comparing models across different datasets or tasks. They argue that Informedness provides a more robust and unbiased measure of model capability.

Overall, the findings of this paper suggest that the choice of evaluation metric can significantly impact model ranking and the understanding of model capabilities. Informedness offers a parsimonious baseline for task performance evaluation in NLP and can be used to compare models across different tasks without the need for dataset re-balancing.

One potential critique of this work is the limited scope of the experiments, which mainly focus on NLP tasks. It would be beneficial to see how the proposed metrics perform in other domains and tasks. Additionally, further analysis and comparison with existing metrics in the literature would strengthen the argument for the use of Informedness as a standard evaluation metric in NLP.

- Classification evaluation metrics in Natural Language Processing (NLP)
- Metrics such as Accuracy, F1-Measure, AUC-ROC
- Informedness as a classification metric
- Confusion matrix and contingency table
- Class prevalence and class bias
- GLUE benchmark for NLP tasks
- Visual Question Answering (VQA) tasks
- Synthetic modeling and simulation
- Disciplines: Natural Language Processing, Machine Learning, Statistics

Citation: Vickers, P., Barrault, L., Monti, E., & Aletras, N. (2024). We Need to Talk About Classification Evaluation Metrics in NLP. arXiv preprint arXiv:2401.03831.