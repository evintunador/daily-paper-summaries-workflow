In this paper, the authors investigate the phenomenon of oversmoothing in Transformer models. Oversmoothing refers to the gradual loss of information and reduction in the expressivity of the model's representations as the number of layers increases. The authors challenge the prevailing belief that Transformers are inherently low-pass filters that oversmooth the inputs, and instead propose that whether oversmoothing occurs or not depends on the eigenspectrum of the attention and weight matrices in the Transformer update equations.
`
The authors provide a new characterization of how the eigenspectrum of attention and weights affects oversmoothing as depth increases. They show that many successful pre-trained Transformer models have attention and weight matrices that satisfy conditions that avoid oversmoothing. They also analyze the closely-related phenomenon of rank collapse and show that it occurs only in rare cases.

Based on their analysis, the authors propose a simple reparameterization of the weights in the Transformer update equations that ensures oversmoothing does not occur. They introduce a symmetric and non-symmetric eigendecomposition of the weight matrix and show that by controlling the eigenvalues of the weight matrix, oversmoothing can be avoided.

The authors conduct experiments on different Transformer models trained on CIFAR100 and ImageNet datasets to validate their findings. They show that their proposed reparameterization improves generalization, even when training with fewer data points or corrupted data. They also demonstrate that the distribution of dominating eigenvalues in the weight matrix varies depending on the training data and architecture.

The implications of this work are significant for the design and optimization of Transformer models. By understanding the role of the eigenspectrum of attention and weight matrices, researchers can control oversmoothing and improve the expressivity of Transformer representations. This can lead to better performance and generalization in various domains.

One potential critique of this work is that the experiments are limited to specific datasets and architectures. Further investigation is needed to validate the findings on a wider range of datasets and models. Additionally, the proposed reparameterization may introduce additional computational complexity, which should be considered in practical applications.

- Understanding of Transformer models and their architecture
- Familiarity with linear algebra concepts such as eigenspectrum, eigenvalues, and eigenvectors
- Knowledge of low-pass filters and their properties
- Basic understanding of machine learning and deep learning concepts

Citation: Dovonon, G., Bronstein, M., & Kusner, M. J. (2024). Setting the Record Straight on Transformer Oversmoothing. arXiv preprint arXiv:2401.04301.