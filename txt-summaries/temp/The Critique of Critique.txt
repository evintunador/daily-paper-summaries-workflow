The paper introduces a framework called METACRITIQUE to evaluate the quality of critiques generated by Large Language Models (LLMs). The goal is to assess the factual accuracy and comprehensiveness of the critiques. METACRITIQUE consists of three steps: reference generation, AIU extraction, and hypothesis critiquing.
`
In the reference generation step, the authors use GPT-4, a powerful language model, to generate reference answers and reference critiques. These generated references serve as proxies for the ground truth, as obtaining human-written references is time-consuming and expensive.

The AIU extraction step involves splitting the critiques into Atomic Information Units (AIUs), which are the smallest units of information that can stand alone. This helps reduce ambiguity in the evaluation process and improves transparency. The authors use GPT-4 to extract AIUs from the critiques.

In the hypothesis critiquing step, the authors evaluate the critiques for precision and recall. Precision measures the factual accuracy of the critiques, while recall measures the coverage of the critiques compared to the reference critiques. The authors prompt GPT-4 to perform these evaluations, using CoT reasoning to guide the model's responses.

To validate the effectiveness of METACRITIQUE, the authors construct a meta-evaluation dataset consisting of question-answer pairs and corresponding critiques from various domains. The dataset includes both human-written critiques and LLM-generated critiques. Human annotators are employed to label the critiques for precision and recall, and the results are compared to the scores obtained using METACRITIQUE.

The results show that METACRITIQUE achieves near-human performance in evaluating the quality of critiques. It can effectively identify high-quality critiques, which can lead to improved refinement of LLMs. This demonstrates the potential of METACRITIQUE to enhance the alignment of LLMs and improve their performance.

Overall, METACRITIQUE provides a principled framework for evaluating the quality of critiques generated by LLMs. It addresses the challenges of quantification, reliability, and intricacy in critique evaluation. The framework is transparent, interpretable, and can be used to assess both human-written and LLM-generated critiques.

- Natural language processing
- Large Language Models (LLMs)
- Precision and recall evaluation metrics
- Factuality detection
- Atomic Information Units (AIUs)
- Chain-of-Thought (CoT) reasoning
- Question answering
- Reasoning
- Entailment
- Summarization

Citation: Sun, S., Li, J., Yuan, W., Yuan, R., Li, W., & Liu, P. (2024). The Critique of Critique: METACRITIQUE. arXiv preprint arXiv:2401.04518.