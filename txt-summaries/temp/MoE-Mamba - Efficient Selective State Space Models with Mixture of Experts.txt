In this paper, the authors propose a model called MoE-Mamba that combines Mixture of Experts (MoE) with the Mamba architecture, a type of State Space Model (SSM). The goal is to leverage the efficiency gains of both MoE and SSMs to scale up sequence modeling tasks.
`=
The Mamba architecture is a type of SSM that offers linear-time inference and efficient training. It uses a selective mechanism to control how information propagates along the sequence dimension, allowing the model to retain essential information while filtering out irrelevant information. Mamba has shown strong performance in various domains and is a promising alternative to the attention-based Transformer architecture.

MoE is a technique that allows scaling up models without significantly increasing computational requirements. It has been successfully applied to Transformers, and the authors propose integrating it with Mamba to further improve its performance. In MoE-Mamba, every other Mamba layer is replaced with a MoE feed-forward layer based on the Switch architecture.

The authors conducted experiments comparing MoE-Mamba with other models, including vanilla Mamba, Transformer, and Transformer-MoE. The results show that MoE-Mamba achieves the same performance as vanilla Mamba in 2.2 times fewer training steps. It also outperforms Transformer-MoE and shows potential gains over the Transformer architecture.

The authors also explored the impact of the number of experts in MoE-Mamba and found that the model scales well with the number of experts. The best performance was achieved with 32 experts.

The paper suggests several future research directions, including scaling MoE-Mamba to larger models, integrating MoE into the Mamba layer itself, and exploring different types of MoE architectures.

Overall, the paper presents MoE-Mamba as a promising approach to scaling up SSMs for sequence modeling tasks. The results demonstrate the potential of combining MoE with the Mamba architecture and highlight the efficiency gains achieved by MoE-Mamba.

- State Space Models (SSMs)
- Mixture of Experts (MoE)
- Transformers
- Sequence modeling
- Linear-time inference
- Selective mechanisms
- Feed-forward layers
- Attention mechanism
- Language modeling
- Control theory

Citation: Pi ´oro, M., Ciebiera, K., Kr ´ol, K., Ludziejewski, J., & Jaszczur, S. (2024). MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts. arXiv preprint arXiv:2401.04081.