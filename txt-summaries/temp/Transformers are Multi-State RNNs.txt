In this research work, the authors propose a new perspective on transformers, a popular type of neural network used in natural language processing (NLP). They argue that transformers can be conceptualized as a type of recurrent neural network (RNN) called Multi-State RNNs (MSRNNs). They show that transformers can be seen as MSRNNs with an infinite number of states, where each state corresponds to a previous token in the sequence. They also demonstrate that pretrained transformers can be converted into finite MSRNNs by applying compression policies that limit the number of tokens processed at each step.
`
The authors introduce a novel compression policy called TOV A, which selects which tokens to keep in memory based on their attention scores. They compare TOV A with existing compression policies such as Window, Window +i, and H2O, and show that TOV A outperforms all other policies in terms of performance on various long-range tasks. They also find that TOV A achieves comparable results to the full (infinite) model while using significantly less memory.

The experiments conducted by the authors focus on evaluating the performance of different compression policies on long-range tasks such as language modeling, long-range understanding, and text generation. They use several benchmark datasets and pretrained transformer models to assess the effectiveness of the policies. The results show that TOV A consistently outperforms other policies and achieves near-comparable results to the full model while using a fraction of the memory.

The implications of this research are twofold. First, it provides a new perspective on transformers, suggesting that they can be understood as a form of RNN. This sheds light on the behavior of transformer models and highlights the similarities between transformers and traditional RNNs. Second, the proposed compression policy, TOV A, offers a more efficient way to reduce memory consumption in pretrained transformers without sacrificing performance. This has practical implications for deploying transformer models in resource-constrained environments.

One potential critique of this work is that the experiments focus on a limited set of long-range tasks and pretrained models. It would be valuable to evaluate the proposed compression policies on a wider range of tasks and models to assess their generalizability. Additionally, the authors acknowledge that their experiments do not include extrapolation to longer sequences, which could be an interesting avenue for future research.

Overall, this research provides valuable insights into the behavior of transformers and offers a practical solution for reducing memory consumption in pretrained models. The findings have implications for both the theoretical understanding of transformers and the practical deployment of transformer models in real-world applications.

- Recurrent Neural Networks (RNNs)
- Transformers
- Attention mechanism
- Language modeling
- Long-range understanding tasks
- Text generation
- Perplexity
- ROUGE metric
- F1 score
- Natural Language Processing (NLP)
- Deep learning
- Machine learning

Citation: Oren, M., Hassid, H., Adi, Y., & Schwartz, R. (2024). Transformers are Multi-State RNNs. arXiv preprint arXiv:2401.06104.