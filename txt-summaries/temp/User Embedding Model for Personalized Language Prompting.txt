This research paper focuses on the challenge of modeling long user histories for preference understanding in natural language. The authors propose a User Embedding Module (UEM) that efficiently processes user history in free-form text by compressing and representing them as embeddings. These embeddings are then used as soft prompts to a language model (LM) to generate personalized recommendations.
`
The experiments conducted by the authors demonstrate the superiority of their approach in handling significantly longer user histories compared to traditional text-based methods. The models trained using the proposed approach show substantial improvements in predictive performance, with up to 0.21 and 0.25 F1 points improvement over the text-based prompting baselines.

The main contribution of this research is the ability to bias language models via user signals. By incorporating longer user histories into the LM through the UEM, the models are better able to understand user preferences and generate recommendations that align more closely with their interests.

The paper also presents several ablation experiments to analyze the impact of different factors on model performance. The results show that increasing the number of history items improves the model's understanding of user preferences. Additionally, the choice of language model and the size of the User Embedding Module also affect performance, with the FlanT5 model and a larger UEM size yielding better results.

It is important to note that while the proposed approach shows promising results, there are limitations to consider. The text prompting can be further optimized using text-to-text prompt compression models. The simplicity of the UEM architecture leaves room for improvement, and different tasks may require different levels of complexity. Additionally, the experiments are conducted using language models with <1B parameters, and it would be interesting to explore larger models with parameter-efficient tuning techniques.

In conclusion, this research provides a novel approach to modeling long user histories for preference understanding in natural language. The proposed User Embedding Module effectively compresses user history into embeddings, enabling the incorporation of longer histories into language models. The results demonstrate improved predictive performance and the ability to generate personalized recommendations based on user preferences. Future work can explore further optimizations and extensions to multimodal inputs.

- Language models (LMs)
- Natural language processing (NLP)
- Recommendation systems
- Transformer networks
- Text-to-text approach
- Soft prompting
- User history modeling
- User embeddings
- Multi-label classification
- Genre extraction
- Machine learning evaluation metrics (e.g., precision, recall, F1 score)

Citation: Sumanth Doddapaneni, Krishna Sayana, Ambarish Jash, Sukhdeep Sodhi, Dima Kuzmin. "User Embedding Model for Personalized Language Prompting." arXiv:2401.04858v1 [cs.CL] 10 Jan 2024.